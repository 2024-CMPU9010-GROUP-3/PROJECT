# User Evaluation
* Goal: gain feedback from real users, learn if product works as expected, assess how user-friendly it is

## Usability testing A(remote un-moderated)
Data:
We are collecting mix of data- quantitative through multiple choice questions and qualitative through open-ended questions

Tools/Techniques:
* Session replay for remote unmoderated is advised
* Contentsquare for user interviews (online)

Structure:
a, b, c = pre-test
d, e = in-test
f, g = post-test
* a. Demographic questions (age range) -> no need to ask more as we already have sector and county
* b. Technology proficiency questions -> is that useful?
* c. Similar product questions
* d. Questions about usability tasks
* e. Specific questions about design, navigation, language
* f. Reflective questions on test & tasks
* g. Questions on overall opinion

Questions:
Dont ask leading questions, ever
* a.
    * What age group are you in ? (18-24, 25-30, 31-40, 41-50, 51-60, 60+)
    * What is your profession?
* b. 
    * What device do you normally use for work/personal?
    * How often do you use this device?
* c.
    * Have you ever used digital mapping tools like Google Maps/Parkopedia?
    * How experienced/comfortable are you with these tools?
    * Which features do you use most in such tools?
* d. & e. 
    * What do you think of the user interface?
    * How is the language used on this page?
    * What's your opinion on the way the features are laid out?
    * How was the process of [task]?
    * Based on the previous task, how would you prefer to do this action instead?
    * How was you experience completing task X?
* f. & g.
    * What was your overall impression of Magpie?
    * How did you find the sign up/ log in process?
    * On a scale of 1-10, how was your experience with the interface?
    * What was the best/worst thing about the application?
    * How would you change the dashboard/landing page/sign up page/profile?
    * How would you describe the overall experience of this test?
    * Do you have any additional comments or questions?

### Testing 1 : Regular user - with tasks
### Testing 2 : Contextual (at someone's job)

## Usability testing B (in-person moderated)
Data:
We are collecting mix of data- quantitative through multiple choice questions and qualitative through open-ended questions

Tools/Techniques:
If the user is asking us questions during testing (to help avoid biais):
* Echo - repeating user's last phrase as a question
* Boomerang - use generic neutral questions (What do you think?)
* Columbo - trailing off on sentence/question so the user can fill the silence by elaborating further

Tips:
* Ask specific questions based on their behavior
* Don't bombard the user though
* Figure out if they are thinking out loud (TAP) or addressing you a question
* Weigh up if asking further questions will actually get more useful information or if observing the user is enough to form a conclusion

Structure & Questions:
Same as above

## Expert review (Andrea)

1. Free-roam the system
2. Give few tasks to perform
3. Ask questions
* How would we make a map accessible to blind/visually impaired users?
* General user experience questions
* Rat