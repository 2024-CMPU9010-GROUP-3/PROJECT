\subsubsection{Backend Technologies}
\paragraph{Go}
At the start of the project, a portion of the team deliberated about which
technologies to choose for the backend. Options using JavaScript such as NodeJS
or Deno with accompanying frameworks like ExpressJS or NestJS were considered,
as was Rust and its Rocket framework. They would have been excellent choices as
they are well established in the industry and tried-and-tested.

The majority of the team has development experience with JavaScript, so going
with that would have made a lot of sense. However, it was planned from the very
beginning to deploy this application on a server hosted by one of the team
members. Therefore, the usage of computing and memory resources was very
important to them, as they did not want to strain their Kubernetes cluster more
than necessary. Since NodeJS runs on the JavaScript runtime V8, which also
powers Google Chrome, our experience indicated that it would be quite resource
intensive to run. Findings by \textcite{Tanadechopon2023} confirm this.

Due to this, the team shifted towards using compiled rather than interpreted
languages, as these are generally more resource efficient. Since small
executables and low memory usage was desired, languages and frameworks that run
on virtual machines such as Java with Spring or C\# with .Net were not deemed to
be viable options. As a result, only Rust and Go were seriously considered at
this point.

Rust offers a small package size, strong memory safety, an excellent
ecosystem and build-tools. The team member that would focus on backend
development had recent experience in writing Rust code. However, Rust
development can be very tricky and time consuming. Additionally, in our
experience Rust has above average compilation times. In a project with a fixed
deadline and the expectation of rapid development, choosing Rust would have been
detrimental. The team recognised that choosing Rust would come with many
drawbacks while offering only few benefits.

This left choosing Go as the logical conclusion. A large drawback that the team
identified with Go was the missing experience in the team. Two team members had
used Go before, but they last used it a few years back. However, since Go syntax
and the languages concepts hadn't changed much since then, it was deemed
possible to quickly get up to speed, much more quickly than Rust would have
allowed. The final decision fell on using Go, as it offered small binaries,
great memory efficiency, a solid ecosystem of libraries and build-tools and a
feature-rich, built-in library for creating REST-APIs. Go also removes a lot of
the pitfalls that Rust suffers from: it has a simple and approachable syntax and
a very low-friction, high-speed development experience. This was also the
deciding factor, since it was made clear to the team that getting an MVP up and
running as quickly as possible was imperative for the project.

\paragraph{PostgreSQL}
The plan for the MVP of Magpie set the simple goal of delivering useful data to
users. To achieve this, the data needs to be stored in a way in which it can be
efficiently accessed during operation. Since the goal of the project was to
provide a tool to professionals, data integrity at every level was an important
consideration. Additional requirements for the data storage solution were good
support for geospatial data, quick retrieval of a large number of points and
ease-of-use.

There is a plethora of database solutions available today. The team considered
the most common types: document databases like MongoDB and multi-model databases
like PostgreSQL (sometimes called RDBMS).

While document databases like MongoDB have their place in the current
development landscape, it soon became clear that SQL-based, multi-model RDBMSs
would be best suited for the job. They offer tried-and-tested performance and
reliability and they are well equipped to guarantee data integrity. But the
deciding reason was the pre-existing experience the lead developer for the
backend had with these types of databases.

After some deliberation, \textbf{PostgreSQL} was chosen as the database solution for this
project. It was combined with the PostGIS extension to add support for
geospatial data and queries. The decision was not cut and dried as most
established multi-model databases (like Oracle DB or MySQL) offer the
functionality that the requirements were asking for. However, the Kubernetes
cluster this project was going to be deployed on already had a fully configured
PostgreSQL server running on it. Making use of pre-existing infrastructure is
the obvious choice in an agile project that had the prime goal of hitting the
ground running.

\begin{listing}[htbp]
  \centering{}
  \begin{minipage}{\textwidth}
  \begin{minted}{sql}
    -- name: GetPointsInRadius :many
    SELECT Id, LongLat::geometry, Type from points
    WHERE ST_DWithin(
      LongLat::geography,
      ST_SetSRID(ST_MakePoint(@longitude::float, @latitude::float), 4326)::geography,
      @radius::float
    ) AND (
      @types::point_type[] IS NULL OR Type = ANY(@types::point_type[])
    );
  \end{minted}
  \end{minipage}
  \caption{An example of a SQL query with annotations used by sqlc}
  \label{listing:sqlc_query_input}
\end{listing}

\paragraph{sqlc}
In modern software development, there are two common ways for an application to
interact with a database via SQL. There is the old school way of writing raw SQL
queries, put them into prepared statements and execute them against the
database. This gives the developer very granular control over the way their
queries are structured and how they run them. But this method requires a
significant amount of work in designing and implementing a translation layer
between the database and the application. Data that the application wants to
send to the database needs to be prepared into a format the database queries
expect. Data that the application wants to retrieve from the database needs to
be parsed back into the data model that's used by the application.

To make interfacing with databases easier, so called ORMs (Object Relational
Models) were developed. These are libraries that the application developer can
include. Database queries are not done in SQL, but in the same programming
language the application is written in. The ORM provides database interface
functions that take in the data in the format the application uses. To actually
perform any queries or make any changes to the database, the ORM runs its own
SQL queries against the database via a compatible driver internally. ORMs
abstract away the direct communication with the database and provide a simpler
wrapper in the programming language that is used anyway. This can make it easier
and quicker to develop an application that makes use of a database, but it takes
away some of the agency that the developer has.

The team wasn't really happy with either of these solutions, so an alternative
called \textbf{sqlc} was chosen. This tool combines the freedom that using SQL gives
developers with the productivity increase that ORMs offer. sqlc flips the
typical ORM workflow on its head. This means, the developer writes standard SQL
queries and schemas, but includes sqlc directives such as the name of the query
and the multiplicity of expected results in the comments above the query (see
Listing \ref{listing:sqlc_query_input}). These files are then passed to the CLI
of sqlc. In accordance with a configuration file provided by the developer, sqlc
then automatically generates type-safe bindings for the queries that were
defined in the SQL files (\cite{sqlc_introduction}).

This approach gives the developer more control about the queries that are
executed. At the same time, it eliminates the need to write boilerplate wrapper
code for accessing the database, just like an ORM would eliminate the need to
write SQL queries. The tool treats SQL, which is a structured and typed
language, as a source of truth. It also enables developers to reuse their
queries across systems as sqlc offers code generation for multiple programming
languages like Go, Python or Typescript and databases like PostgreSQL, MySQL or
SQLite (\cite{sqlc_documentation_language_support}).

The tool respects best practices to prevent SQL injection attacks. It generates
code that only utilises constant strings and parametrised queries
(\cite{sqlc_injection}). Using this tool, the team was quickly able to
connect the backend server to the database. Eliminating the need to update the
Go code manually each time the database schema or queries changed was vital for
the backend keeping pace with the other developers.

\begin{listing}[htbp]
  \centering{}
  \begin{minipage}{0.75\textwidth}
  \begin{minted}{yaml}
      version: "2"
      sql:
      - schema: "sql/migrations"
        queries: "sql/queries_private.sql"
        engine: "postgresql"
        gen:
          go:
            package: "db"
            sql_package: "pgx/v5"
            out: "internal/db/private"
            build_tags: "private"
            emit_json_tags: true
            overrides:
              - db_type: "geometry"
                go_type:
                  import: "github.com/twpayne/go-geom"
                  pointer: true
                  type: "Point"
      - schema: "sql/migrations"
        queries: "sql/queries_public.sql"
        engine: "postgresql"
        gen:
          go:
            package: "db"
            sql_package: "pgx/v5"
            out: "internal/db/public"
            build_tags: "public"
            emit_json_tags: true
            overrides:
              - db_type: "geometry"
                go_type:
                  import: "github.com/twpayne/go-geom"
                  pointer: true
                  type: "Point"
  \end{minted}
  \end{minipage}
  \caption{An example of a sqlc configuration file with two targets with separate query inputs and type replacement}
  \label{listing:sqlc_config_file}
\end{listing}

\begin{listing}[htbp]
  \centering{}
  \begin{minipage}{0.85\textwidth}
  \begin{minted}{go}
    const getPointsInRadius = `-- name: GetPointsInRadius :many
    SELECT Id, LongLat::geometry, Type from points
    WHERE ST_DWithin(
      LongLat::geography,
      ST_SetSRID(ST_MakePoint($1::float, $2::float), 4326)::geography,
      $3::float
    ) AND (
      $4::point_type[] IS NULL OR Type = ANY($4::point_type[])
    )
    `

    type GetPointsInRadiusParams struct {
      Longitude float64     `json:"longitude"`
      Latitude  float64     `json:"latitude"`
      Radius    float64     `json:"radius"`
      Types     []PointType `json:"types"`
    }

    type GetPointsInRadiusRow struct {
      ID      int64          `json:"id"`
      Longlat *go_geom.Point `json:"longlat"`
      Type    PointType      `json:"type"`
    }

    func (q *Queries) GetPointsInRadius(
        ctx context.Context, arg GetPointsInRadiusParams
      ) ([]GetPointsInRadiusRow, error) {
      rows, err := q.db.Query(ctx, getPointsInRadius,
        arg.Longitude,
        arg.Latitude,
        arg.Radius,
        arg.Types,
      )
      if err != nil {
        return nil, err
      }
      defer rows.Close()
      var items []GetPointsInRadiusRow
      for rows.Next() {
        var i GetPointsInRadiusRow
        if err := rows.Scan(&i.ID, &i.Longlat, &i.Type); err != nil {
          return nil, err
        }
        items = append(items, i)
      }
      if err := rows.Err(); err != nil {
        return nil, err
      }
      return items, nil
    }
  \end{minted}
  \end{minipage}
  \caption{An example of a Go binding generated by sqlc from the SQL query in
  Listing \ref{listing:sqlc_query_input}}
  \label{listing:sqlc_generated_bindings}
\end{listing}


\paragraph{golang-migrate}

During the first weeks of the project, the database was defined by a simple
initialisation SQL script. This worked fine for the time and allowed all
developers to set up their local database instance easily. But once the vertical
slice was completed, the team wanted to add additional features that would
necessitate changes to the database schema. After adding the first feature, some
developers ran into issues where the version of the backend they were running
was not compatible with the schema currently deployed on their local database.

Fixing these issues was time consuming and they were likely going to reappear
after future database changes. Realising this, the team made the decision to
integrate a database migration solution to automate this tedious process and
prevent future time losses during development.

The tool used for generating Go bindings from the raw SQL, sqlc, does not offer
any migration functionality, so another solution needed to be found. The team
evaluated all options that were listed as compatible with sqlc. These were
atlas, dbmate, golang-migrate, goose, sql-migrate and tern
(\cite{sqlc_updating_database_schema}). The team was looking for a solution that
allowed the migrations to be specified in plain SQL. The solution needed to work
both as a command-line interface application as well as a Go library, as it was
deemed necessary to apply the migrations to the database automatically. A tool
that did not rely on additional configuration files would also be preferred.
Multiple options satisfied these requirements. The backend team decided to use
\textbf{golang-migrate}, due to its straightforward usage and easy integration as a
library.

At the time this decision was taken, golang-migrate also seemed like
the most actively developed of the options. This has since changed and at the
time of writing, golang-migrate is the solution with the least recent update of
the options.

Nonetheless, using golang-migrate the team was able to integrate automated
database migrations into the app, eliminating the need to spend valuable time
performing manual database updates each time the schema is changed.

\textbf{The complete database development workflow for the backend server is visualised
in Figure \ref{fig:database_workflow_diagram}.}

\begin{figure}[htbp]
  \centering{}
  \includegraphics[width=0.8\textwidth]{../d2-diagrams/database-workflow/database_workflow.png}
  \caption{Database Workflow Diagram}
  \label{fig:database_workflow_diagram}
\end{figure}

\paragraph{bcrypt}
To store the users credentials securely, \textbf{bcrypt} was chosen to hash them
before they are inserted into the database. The resulting hashes (see Figure
\ref{fig:bcrypt_hash}) can then be used to validate if the given password is
correct. In accordance with best practices, a work factor of 12 was used when
hashing passwords and a password limit of 72 bytes was implemented
(\cite{owasp_password_storage_cheatsheet}).

While the \textcite{owasp_password_storage_cheatsheet} recommends other, more
recent technologies over bcrypt, they also don't discourage its use in any way.
In the case of Magpie, bcrypt was chosen because the team had past experience
working with it. This was desirable as the registration and login functionality
was deemed to be one of the core features of the MVP. Choosing a technology that
the team did not have any experience could have resulted in slower development,
bugs or even security vulnerabilities as a result of improper implementation.

Since the application was not intended to store highly valuable personal data
such as banking information, personal addresses or health care records it was
deemed acceptable to use a less recent technology in exchange for more rapid
development. Should this project ever evolve into a publicly available and
frequently used application, the use of bcrypt would of course need to be
reevaluated with a much bigger focus on real-world application security rather
than just development speed.

\begin{figure}[H]
  \centering{}
  \includegraphics[width=0.8\textwidth]{./images/bcrypt_hash.png}
  \caption{Visualisation of a bcrypt hash}
  \label{fig:bcrypt_hash}
\end{figure}

\paragraph{JSON Web Token} \label{jwt}
\textbf{JSON Web Tokens} were chosen for the authentication of user requests to
the APIs. JWTs are JSON objects that are encrypted using a private key on the
server and passed to the frontend when a login requests succeeds. The client is
then expected to pass the token to the backend as a form of authentication in
subsequent requests.

JSON Web Tokens are more than just a simple API token. They can store data about
the authenticated user in them, which can be very useful if the handling of the
requests needs a reference to the user id for example (see example JWT content
in Figure \ref{}). Since the tokens are generated on the backend server using a
private secret, JWTs can be checked for integrity during decoding. Should the
content not match up with the private secret of the backend server, it can be
assumed that the token has been tempered with. In this case, the request would
be discarded and a "Unauthorized" error message would be returned instead.

This structure allows for the authentication system to be decoupled between
backend and frontend. Using JWTs eliminates the need for the backend to keep
track of active sessions, significantly reducing complexity. This and the
familiarity the team had with working with JWTs were ultimately chosen as the
authentication solution for Magpie.

JSON Web Tokens do however come with their own caveats. For example, if any
malicious actor stole such a token from a user, they could pretend to be that
user without the system noticing. JWTs do not have a built-in way to revoke
potentially stolen tokens, neither from the client nor from the server
(\cite{owasp_jwt_cheatsheet}).

Despite these caveats and some slowdowns during development, the team still
feels JWT-based authentication was the right choice for a project of Magpie's
scale.

\subsubsection{Backend Development}
As discussed in \ref{systemarchitecture} \nameref{systemarchitecture}, the
backend is comprised of two standalone REST-API servers. Right after the
decision to split the backend was taken, a first implementation using two
separate codebases was created. This approach was functional, meaning it was
able to produce two distinct backends with different feature sets.

But it soon became obvious that this approach was not sustainable. While simply
splitting the backend into two codebases was a quick and easy solution, it would
almost certainly lead to significantly increased development times in the
future. The two backends have a fair amount of identical functionality and
differ just in the route handlers and the database queries. Leaving the
codebases separate would result in many changes being made twice -- once in the
private backend and once in the public backend.

After careful consideration and discussions with the team, the decision was made
to revert the change that split the backend. The desired result of two distinct
backends would have to be achieved in a different way.

To accomplish this, a feature of the Go programming language called
\textbf{build-tags} was utilised. Usage examples of this feature showcase it by
creating multiple binaries that have different feature sets, for instance
multiple different payment tiers for a single software. This was a great
solution for this problem. It allowed for certain files to be excluded during
compilation based on if they were needed for private or public functionality.
While the development of a program split by build-tags is more challenging
than developing a single binary, it is much more streamlined than keeping
functionality identical between two separate projects.

The build-tag system did not fix the problem of code duplication completely
however. Some tools in the Go ecosystem did not handle the exclusive split as
expected. Most notably, the Go language server that provides code completion and
other useful IDE features. It was possible to configure it to recognise files
with their accompanying build-tags, but there was no way to specify exclusivity.
This meant that two files in the same package could not have identical function
definitions even though they would never be included in the binary build process
simultaneously. It was possible to work around this restriction, which it lead
to some code duplication. But this was still a lot better than duplicating the
whole backend.

And while sqlc offers integration for the build-tag system (see the config file
in Listing \ref{listing:sqlc_config_file}), it did not offer to just split the
query part of the generated bindings. As a consequence, the generated code
handling the model bindings and the access to the database had to be generated
and included twice.

\subsubsection{Database}
\fcolorbox{orange}{orange}{Add database development}
don't forget down migration point type enum

\begin{figure}[htbp]
  \centering{}
  \includegraphics[width=\textwidth]{../d2-diagrams/database-init/database-init.png}
  \caption{Database Schema for the minimum viable Product}
  \label{fig:database_init_schema}
\end{figure}

\begin{figure}[htbp]
  \centering{}
  \includegraphics[width=\textwidth]{../d2-diagrams/database-final/database-final.png}
  \caption{Final Database Schema}
  \label{fig:database_final_schema}
\end{figure}

\subsubsection{Routing}
Routing the frontend's requests to the appropriate handlers was done using the
\textbf{net/http} package, which is included in the Go standard library. The Go
ecosystem provides a number of alternative solutions such as Gin, Gorilla or
Fibre, which are mature and well documented frameworks for creating APIs. They
provide many useful features, which the team acknowledged during their
evaluation, but -- in the team's opinion -- they can also easily add a lot of
complexity. Since the team was lacking in recent Go knowledge and didn't want to
unnecessarily overcomplicate the project, it was decided to go with the built-in
solution instead.

To realise the API functionality, the http server function provided by the
net/http library is configured with a \texttt{http.ServeMux} object. This is
where the server will send all incoming requests. These are then passed to
nested routers based on prefix routing. For example, a request for the route
\texttt{/v1/public/auth/User/login} would follow this path: HTTP Server
$\rightarrow$ Public Router $\rightarrow$ Public Auth Router $\rightarrow$ Auth
handler. A full overview of the routing in the backend can be found in Figure
\ref{fig:backend_routing}.

All requests are also routed through a stack of middlewares. Some middlewares
are active on all requests (Logging, CORS) and some are configured based on
route (Authorisation), see Section \ref{backend_middlewares}
(\nameref{backend_middlewares}).

\begin{figure}[htbp]
  \centering{}
  \includegraphics[width=\textwidth]{../d2-diagrams/backend-routing/backend-routing.png}
  \caption{Routing in the Backend: from Request to Handler}
  \label{fig:backend_routing}
\end{figure}

\begin{listing}[htbp]
  \begin{minted}{go}
  func init() {
    AddRoute(route{"/public/", public()})
  }

  func public() *http.ServeMux {
    router := http.NewServeMux()
    router.Handle("/points/", http.StripPrefix("/points", pointsPublic()))
    router.Handle("/auth/", http.StripPrefix("/auth", auth()))
    router.Handle("/history/", http.StripPrefix("/history", locationHistory()))
    return router
  }

  func pointsPublic() *http.ServeMux {
    router := http.NewServeMux()
    handler := &handlers.PointsHandler{}

    // Authenticated access
    router.Handle("GET /inRadius", Authenticated(http.HandlerFunc(handler.HandleGetByRadius)))
    router.Handle("GET /{id}", Authenticated(http.HandlerFunc(handler.HandleGetPointDetails)))

    return router
  }

  func auth() *http.ServeMux {
    router := http.NewServeMux()
    handler := &handlers.AuthHandler{}

    // Public access
    router.Handle("POST /User/login", Public(http.HandlerFunc(handler.HandleLogin)))
    router.Handle("POST /User/", Public(http.HandlerFunc(handler.HandlePost)))

    // Protected access
    router.Handle("GET /User/{id}", Protected(http.HandlerFunc(handler.HandleGet)))
    router.Handle("PUT /User/{id}", Protected(http.HandlerFunc(handler.HandlePut)))
    router.Handle("DELETE /User/{id}", Protected(http.HandlerFunc(handler.HandleDelete)))

    return router
  }

  func locationHistory() *http.ServeMux {
    router := http.NewServeMux()
    handler := &handlers.LocationHistoryHandler{}

    // Protected access
    router.Handle("GET /{id}", Protected(http.HandlerFunc(handler.HandleGet)))
    router.Handle("DELETE /{id}", Protected(http.HandlerFunc(handler.HandleDelete)))
    router.Handle("POST /{id}", Protected(http.HandlerFunc(handler.HandlePost)))

    return router
  }

  \end{minted}
  \caption{An example of how routing is configured in the backend}
  \label{listing:routing_example}
\end{listing}

\subsubsection{APIs}
All API routes that the backends expose are listed below. This information was
available to the frontend and data stack developers throughout the project in
the form of a collection of markdown files in the repository. This clearly
communicated the capabilities of the API routes, making it easier to develop the
application parts that interfaced with them. Providing strong documentation also
eliminated a lot of ambiguity, resulting in more concise and effient
communication between the different areas of responsibility.

For a detailed explanation of the route access restriction modes, see Section
\ref{route_authorisation} (\nameref{route_authorisation}).

\paragraph{Public API}
The public API is used to retrieve relevant data for display in the frontend and
for authentication of the user.

\begin{itemize}
  \item{
    \textbf{Point Routes}
    \begin{itemize}
      \item { \textbf{Name:} Point Details\\
          \textbf{Method + Route:} \texttt{GET /v1/public/points/\{id\}}\\
          \textbf{Description:} Provides additional details about a point. The
          structure and content of the details is different for every point
          type. The details are derived from the input dataset and added to each
          point on ingress.\\
          \textbf{Access:} Authenticated\\
          \textbf{Parameters:} Point UUID (path parameter)\\
          \textbf{Returns:} JSON Object containing details about the point\\
        }
      \item { \textbf{Name:} Points in Radius\\
          \textbf{Method + Route:} \texttt{GET
          /v1/public/points/inRadius?long=\{\}\&lat=\{\}\&radius=\{\}\&types=\{\}}\\
          \textbf{Description:} Retrieves all points in a circle. The circle is
          defined by its center coordinates and a radius in meters. The
          \texttt{types} parameter is optional. If a comma separated list of
          point types is provided, the returned data will be filtered to only
          contain points with the given types.\\
          \textbf{Access:} Authenticated\\
          \textbf{Parameters:} Longitude, Latitude, radius, types (all query
          parameters)\\
          \textbf{Returns:} JSON object containing a list of points inside the
          given radius, filtered by the given point types\\
        }
    \end{itemize}
  }
  \item{
    \textbf{Saved Location Routes}
    \begin{itemize}
      \item {
        \textbf{Name:} Retrieve saved locations\\
        \textbf{Method + Route:} \texttt{GET /v1/public/history/\{id\}}\\
        \textbf{Description:} Retrieves all saved locations for the account
        specified by the user id. In previous versions, this route included
        pagination functionality, but due to the requirements of a frontend
        library, the backend now returns all saved locations.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} User UUID (path parameter)\\
        \textbf{Returns:} JSON object containing a list of all saved locations
        (including location, radius, number of amenities) for the specified
        user.\\
      }
      \item {
        \textbf{Name:} Delete saved locations\\
        \textbf{Method + Route:} \texttt{DELETE /v1/public/history/\{id\}}\\
        \textbf{Description:} This route allows the user to delete one or more
        of their saved locations. The route only deletes saved locations that
        are associated with the currently active account. Should this route
        receive ids of saved locations that are associated with another account
        or that don't exist, it simply ignores them.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} User UUID (path parameter), List of saved location
        ids (request body)\\
        \textbf{Returns:} Status 202 if no errors occurred during deletion.\\
      }
      \item {
        \textbf{Name:} Save location\\
        \textbf{Method + Route:} \texttt{POST /v1/public/history/\{id\}}\\
        \textbf{Description:} Creates a new saved location for the currently
        active user.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} User UUID (path parameter), JSON Object describing
        the saved location (request body)\\
        \textbf{Returns:} Status 201 if the location was saved successfully.\\
      }
    \end{itemize}
  }
  \item{
    \textbf{Authentication Routes}
    \begin{itemize}
      \item {
        \textbf{Name:} Create new account\\
        \textbf{Method + Route:} \texttt{POST /v1/public/auth/User/}\\
        \textbf{Description:} Allows users to create a new account. Both the
        backend and database will perform uniqueness checks on the given email
        and username. First name and last name are optional. The given password
        will be hashed before being stored in the database.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} JSON object containing all information for the new
        account (request body)\\
        \textbf{Returns:} The UUID of the new account and status 201 if the
        creation was successful.\\
      }
      \item {
        \textbf{Name:} Login\\
        \textbf{Method + Route:} \texttt{POST /v1/public/auth/User/login}\\
        \textbf{Description:} Allows users to retrieve an access token, given
        valid credentials.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} JSON object containing a password and a username or
        email.\\
        \textbf{Returns:} A JWT bearer token and the user UUID if the login was
        successful.\\
      }
      \item {
        \textbf{Name:} Get user details\\
        \textbf{Method + Route:} \texttt{GET /v1/public/auth/User/\{id\}}\\
        \textbf{Description:} Retrieves the user details for the current user.
        This includes the users first name, last name, date of registration,
        date of last login as well as a link to a profile picture. Requests for
        the details of other users are denied.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} User UUID (path parameter)\\
        \textbf{Returns:} JSON object containing additional details about the
        user.\\
      }
      \item {
        \textbf{Name:} Update user details\\
        \textbf{Method + Route:} \texttt{PUT /v1/public/auth/User/\{id\}}\\
        \textbf{Description:} Allows the user to update their own user details.
        With the exception of a new password, all parameters must be passed to
        this route, even if they aren't changed. New usernames and emails are
        checked for uniqueness by the backend and database. Keeping the same
        username or email does not violate the uniqueness constraint.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} JSON object specifying username, email, [password],
        first name, last name and profile picture link.\\
        \textbf{Returns:} Status 202 if the data was successfully updated.\\
      }
      \item {
        \textbf{Name:} Delete account\\
        \textbf{Method + Route:} \texttt{DELETE /v1/public/auth/User/\{id\}}\\
        \textbf{Description:} Allows the user to delete their account. This also
        deletes all associated data like user details and saved locations. There
        is currently no way to undo this action.\\
        \textbf{Access:} Restricted\\
        \textbf{Parameters:} User UUID (path parameter)\\
        \textbf{Returns:} Status 202 if the request was accepted.\\
      }
    \end{itemize}
  }
  \item{
    \textbf{Status Routes}
    \begin{itemize}
      \item {
        \textbf{Name:} Heartbeat\\
        \textbf{Method + Route:} \texttt{GET /heartbeat}\\
        \textbf{Description:} This route is used by the automated deployment to
        check if the public backend is up and ready to receive connections. This
        helps alleviate issues with the starting order of services on a
        Kubernetes cluster.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} None\\
        \textbf{Returns:} \texttt{true} if the service is ready to receive
        connections.\\
    }
    \end{itemize}
  }

\end{itemize}

\paragraph{Private API}
The private API is used by the machine learning and data stack to insert new
datasets into the database. Currently, all private API routes have public access
rights, as the private backend as a whole is guaranteed restricted access by the
CoreDNS routing of the Kubernetes cluster. This allows the data stack to skip
the authentication step, decreasing development overhead while maintaining high
application security.

\begin{itemize}
  \item {
    \textbf{Point Routes}
    \begin{itemize}
      \item {
        \textbf{Name:} Create new point\\
        \textbf{Method + Route:} \texttt{POST /v1/private/points}\\
        \textbf{Description:} Allows the data stack to create a new point in the
        database. Since the point details vary from point type to point type,
        they are stored in the database in the form of a JSON object. This also
        allows for less restrictions to the type of data that can be added here,
        making Magpie more flexible in development.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} JSON object containing the location of the point,
        the point type and a nested JSON object containing all the point
        details.\\
        \textbf{Returns:} The integer id of the newly created point.\\
      }
      \item {
        \textbf{Name:} Update point\\
        \textbf{Method + Route:} \texttt{PUT /v1/private/points/\{id\}}\\
        \textbf{Description:} This route allows the data stack to update a point
        that is already stored in the database. This is not currently used in
        Magpie, but it will be useful in the future as datasets will become
        outdated over time.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} Point id (path parameter), JSON object containing
        the points new data\\
        \textbf{Returns:} The point id and Status 202 if the update was
        successful.\\
      }
      \item {
        \textbf{Name:} Delete point\\
        \textbf{Method + Route:} \texttt{DELETE /v1/private/points/\{id\}}\\
        \textbf{Description:} Using this route, the data stack can delete a
        specific point from the database. This is currently not used in Magpie,
        but removing stale and outdated data will become necessary as time goes
        by.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} Point id (path parameter)\\
        \textbf{Returns:} Status 202 if the request was accepted.\\
      }
    \end{itemize}
  }
  \item{
    \textbf{Status Routes}
    \begin{itemize}
      \item {
        \textbf{Name:} Heartbeat\\
        \textbf{Method + Route:} \texttt{GET /heartbeat}\\
        \textbf{Description:} This route is used by the automated deployment to
        check if the public backend is up and ready to receive connections. This
        helps alleviate issues with the starting order of services on a
        Kubernetes cluster.\\
        \textbf{Access:} Public\\
        \textbf{Parameters:} None\\
        \textbf{Returns:} \texttt{true} if the service is ready to receive connections.\\
      }
    \end{itemize}
  }
\end{itemize}

\subsubsection{Middlewares}\label{backend_middlewares}
\paragraph{Authentication}

\fcolorbox{orange}{orange}{Add authentication middleware features}

As mentioned in section \ref{jwt} (\nameref{jwt}), the use of JWTs comes with
its own set of caveats.

One solution for this issue would be implementing a list of revoked tokens on
the server and offering the user a way to add a stolen token to that list. If an
attacker then uses one of these revoked tokens, their requests can be easily
denied. However, this added complexity and development time that the team didn't
have in the MVP phase of the project. So to mitigate the issue, the JWTs were
given a very short expiration time, making them useless after only a few hours.
This led to annoyance and friction during development and testing, so the
expiration time was increased to a week. This made the application less secure,
but much easier to use in both development and regular usage. The backend team
took a unanimous decision to keep the token handling in this state for the time
being, as the application was still considered very much secure enough. The time
and effort needed to implement those additional security features were weighed
up and it became obvious that they would not result in any easily perceptible
improvement to the application.

The issue of improving application security was kept in mind during the whole
development process, but other issues always took precedence. Given more time,
improving the application security would be one of the top priorities in terms
of further backend development.

\paragraph{Authorisation}\label{route_authorisation}

\fcolorbox{orange}{orange}{Add route based authorisation}

\subsubsection{Data Validation}
\fcolorbox{orange}{orange}{Add section on data validation + DTOs}

\subsubsection{Error Handling}
\fcolorbox{orange}{orange}{Add section on error handling}

\subsubsection{Testing}
\fcolorbox{orange}{orange}{Add section on testing}