\subsubsection{Data collection process}
The Mapbox Static Images API was used to retrieve all the satellite images used in this project.

Initially, a training set of 250 satellite images was collected to train the Yolo model.

Our project has two main Python scripts, \texttt{parking\_detection.py} and \texttt{parking\_detection\_local.py} to localize parking spots.
In \texttt{parking\_detection.py}, the satellite images (Mapbox Satellite) and corresponding road mask images (Mapbox Streets) are retrieved for a specific area contained within a bounding box, defined by its top-left and bottom-right coordinates, directly from the Mapbox API.
From the coordinates of the specific bounding box, the center coordinates of all the images necessary to make up that area are calculated, and the images are then retrieved.
While in \texttt{parking\_detection\_local.py}, the original parking detection script is adapted to run on a database of locally stored Mapbox Satellite images and the corresponding Mapbox Streets images, covering the entirety of Dublin city.
A Mapbox Satellite and Mapbox Streets image can be seen below in Figure~\ref{fig:Mapbox_images}.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/satellite_img.png}
        \caption*{Mapbox Satellite Image}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/streets_img.png}
        \caption*{Mapbox Streets Image}
    \end{minipage}
    \caption{Images retrieved from the Mapbox Static Images API}
    \label{fig:Mapbox_images}
\end{figure}

Extensive testing was done at all the different stages of development of the scripts to identify the optimal thresholds and parameters using test images from Mapbox, in a variety of scenarios including edge cases or cases prone to causing issues.

\subsubsection{Yolo model used for car detection}


\subsubsection{Parking spot detection}
Subsequently, to identify the parking spots in each satellite image, the cars detected by the Yolo model are classified into on the road or parked, based on a road mask.

\subsubsection{Road mask generation}
Many different iterations of the road mask were used, the main iterations and their differences are explained and showed below in Figure~\ref{fig:Mask_Iterations}.

The original mask was a very simple binary mask which identified the road pixels by their color, as the majority of roads were depicted in white and darkened the rest of the pixels in the image.

Through testing and thoroughly analysing the Mapbox Streets images, motorways and national roads were discovered to be depicted in yellow or orange. Therefore, two additional color masks (for orange and yellow) were concatenated to the original binary mask through bitwise operations.

To refine the mask, and reduce misclassification errors, additional annotations contained on the Mapbox Streets images such as street names and white dotted lines which denote a multitude of things (foot paths, planter boxes, pedestrian crosswalks, football fields delimitations, etc. ) were removed through morphological operations, leaving only a plain white line for each road.
A number of different kernel sizes, different contour thresholds and other parameters were tested to achieve the optimal removal of the street names and additional annotations.

A different approach using Canny Edge Detection was tested, however due to the nature of the Mapbox Streets images, the edges picked up were not significant and the overall performances was much worse than the current mask at the time.

A final improvement to the mask was made to avoid certain misclassification due to the Mapbox Streets road width not correctly reflecting all the lanes of the road and therefore classifying on the road cars as parked. This issue was most prominent for motorways and national roads and was resolved by enlarging those roads using a dilation technique. Multiple different kernel sizes, different number of iterations and kernels of different sizes applied consecutively were tested to find the optimal solution.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/streets.png}
        \caption*{Mapbox Streets Image}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/old_mask.png}
        \caption*{Initial Mask}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/new_mask.png}
        \caption*{Mask with additional color masks}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/canny_edge_mask.png}
        \caption*{Canny Edge Detection Mask}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/current_mask.png}
        \caption*{Current Mask}
    \end{minipage}
    \caption{Different Iterations of the Road Mask}
    \label{fig:Mask_Iterations}
\end{figure}

\newpage

\subsubsection{Classification of cars into on the road or parked}
The cars detected by the Yolo model are then sorted into on the road or parked based on the road mask previously generated, which is explained in more detail below.

The model's predictions which are lower than the confidence threshold of 0.4 are discarded as they are more likely to be misclassifications, such as chimneys which were often misclassified as cars.

A bounding box for each of the model's predictions is computed from the center pixel coordinates, the width and the height returned from the model.
The overlap between the bounding box and the road pixels is calculated, and if the overlap is higher than the threshold of 0.5, the prediction is discarded, keeping only the parked cars.
Different thresholds were tested as well, however overall a harsher threshold works better given that these detections are used later on for the empty parking detection.

For each of the model's predictions for parked cars, the center pixels coordinates are mapped to geographic coordinates (longitude and latitude), and the width, height, rotation and orientation based on the angle are saved.

For testing and debugging purposes, all the bounding boxes of the cars found by the model are drawn onto the satellite image, on the road cars are drawn in blue while parked cars are drawn in red as seen in Figure~\ref{fig:Road_mask_classification}.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/road_mask_classification1.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/road_mask_classification2.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/road_mask_classification3.png}
    \end{minipage}
    \caption{Classification of cars into on the road or parked}
    \label{fig:Road_mask_classification}
\end{figure}

\newpage

\subsubsection{Empty parking spot detection}
Subsequently, empty parking spots are detected between parked cars in each image, based on the parked cars detected by the Yolo model and classified using the road mask.
Originally 4 cases were considered, cars parked horizontally in a row or in a column, cars parked vertically in a column and cars parked vertically side by side. However horizontally parked in a column and vertically parked side by side were removed as they lead to too many misclassifications, namely the vertically parked side by side lead to adding many additional cars in driveways in residential areas.
A number of different variations and calculation techniques were tested out, however only the final version will be explained below.

Average parking spot width and length in pixels are calculated for each image, to draw the empty spots more uniformly. Average parking spot width and length in meters are set to 3.05 as found to be the optimal value through testing accounting for the variations in size.
Setting this value avoids misclassifications as previously the average width and length in meters were calculated dynamically, however in cases where the averages were lesser than 3, spots tended to overlap.

The parked cars detected are then sorted by latitude for horizontally aligned cars and by longitude for vertically aligned cars, to identify gaps between consecutive cars.
For each pair of consecutive spots, the distance in meters in between them is calculated. The gap is adjusted to account for the half cars on both sides, as the coordinates of the parked car represent the center of the car.
For the gaps smaller than the maximum gap threshold of 12 meters and larger than the average size of a car, where there is enough space to fit 1 or more car, and where the angle deviation is smaller than a threshold of 35 degrees, to ensure the spots are sufficiently aligned, the number of cars that fit into the gap is calculated.
Based on the number of empty spots found, the coordinates for those empty spots are calculated to be aligned with the 2 consecutive cars and then added to a list of empty spots detected.

Afterwards, duplicate spots defined as spots that overlap too much and where the distance between 2 spots is smaller than a threshold of 1 meter are removed.
As well, spots coinciding with cars identified by the Yolo model, where the distance between them is smaller than 1.25 meters are removed from the empty spots found.
Subsequently, empty spots that overlap with the road are filter out in a similar manner to the classification done by the road mask.

Following the detection of all the empty spots, the bounding boxes corresponding to each spot are drawn onto the satellite image in green as shown in Figure~\ref{fig:Empty_parking_detection}.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/empty_parking1.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/empty_parking2.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/empty_parking3.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/empty_parking4.png}
    \end{minipage}
    \caption{Empty Parking Spot Detection}
    \label{fig:Empty_parking_detection}
\end{figure}

\newpage

\subsubsection{Classification of all spots detected}
All the parking spots detected, including the parked cars identified by the model and the empty parking spots are then classified into 3 different classes: public (on the street parking), private (residential parking) and parking lot, based on their proximity to the nearest road.

Multiple different clustering approaches were tested out to cluster spots together, such as DBSCAN, HDBSCAN and OPTICS.
Overall DBSCAN performed the best and the quickest out of all the clustering algorithms, given that some images contained very few parking spots, meaning clustering was only significant in cases with a minimum number of parking spots.
Through testing, the optimal hyperparmeters found for DBSCAN are \texttt{eps} : 55 and \texttt{min\_samples} : 5, which allow the correct identification of the clusters, avoiding misclassifications of residential areas with many cars as parking lots, which was a common misclassification initially, when the hyperparameters and thresholds were not set correctly.
\texttt{eps} refers to the maximum distance between two points for them to be considered as neighbors and \texttt{min\_samples} refers to the minimum number of points required to form a cluster.
Using HDBSCAN and OPTICS required a minimum number of samples larger or equal to 2, however given that not all images necessarily contained at least 2 parked cars (some images may contain only on the road cars for example), clustering using those algorithms was not possible in all cases, therefore -1 was assigned to the spots belonging to no clusters similarly to the default behaviour of DBSCAN.
Clustering using HDBSCAN gave similar results to DBSCAN, however not as good, while OPTICS only found smaller clusters which were not as significant and not particularly helpful to subsequently classify the spots into the parking lot category.

Parking spots are classified as public if the proximity to the nearest road is less than a pixel threshold of 30, otherwise the parking spots are considered private. This optimal threshold was found through extensive testing on a test set containing various edge cases.
Parking spots are classified as parking lots, when clusters of 18 or more spots are identified in an image. Larger clusters are prioritized as parking lots containing less spots are not as relevant to the target user and given that the threshold of 18 spots minimizes the risk of misclassifying residential areas as parking lots.

Afterwards, clusters and classification labels are drawn onto the satellite image. The clusters are color coded and denoted as a circle at the center of the bounding box, while the classification label is written to the right of the bounding box. The private class in written in red, public in green and parking lot in white as seen below in Figure~\ref{fig:classification_all_spots}.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/classification1.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/classification2.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/classification3.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/classification4.png}
    \end{minipage}
    \caption{Classification of all spots into private, public and parking lot}
    \label{fig:classification_all_spots}
\end{figure}

\newpage

\subsubsection{Uploading points to the backend server}
The longitude, latitude and classification of all the parking spots detected are then saved to a csv file.
Potential spot duplicates are dropped, as the same car may be in multiple images given that the images are defined by their center coordinates, which causes adjacent images to overlap and share common areas. The rightmost part of one image can overlap with the leftmost part of another image.

Subsequently, the parking zones and costs associated to each parking spot found are added to the csv file.
Dublin City Council defines different parking zones by color, based on their high to moderate demand which changes the price per zone as seen below in Figure~\ref{fig:Parking_zones}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Parking_zones_map.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Parking_zones_cost.png}
    \end{minipage}
    \caption{Parking zones and cost defined by Dublin City Council}
    \label{fig:Parking_zones}
\end{figure}

The csv file containing the longitude, latitude, classification label, parking zone and parking cost is then uploaded to the PostgreSQL database, in the backend server using the \texttt{send\_points.py} script.

\subsubsection{Evaluation of each submodule of the parking detection model}
Each major section of the parking detection model was evaluated individually in order to access the overall performance. The results will be presented and analysed for each section below.

\subsubsection{Evaluation of the Yolo model}


\subsubsection{Evaluation of the classifiction of cars into on the road or parked}
The classification of cars into on the road or parked by the road mask is evaluated in the \texttt{evaluate\_road\_mask.py} Python script.
The classification is evaluated on the following metrics commonly used for object detection tasks: Average Intersection over Union (IoU), Balanced Accuracy and Precision, Recall, F1 Score, Accuracy, Specificity per class.

These metrics are defined in detail as follows.

The IoU is calculated as the overlap between the predicted bounding box \( B_p \) and the ground truth bounding box \( B_g \), divided by their union:

\[
\text{IoU} = \frac{|B_p \cap B_g|}{|B_p \cup B_g|}
\]

The Average IoU is defined as the mean IoU across all detections:

\[
\text{Average IoU} = \frac{1}{N} \sum_{i=1}^{N} \text{IoU}_i
\]

where \( N \) is the total number of detections.

Precision is defined as the ratio of true positives (TP) and the sum of true positives and false positives (FP):

\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]

Recall is defined as the ratio of true positives (TP) and the sum of true positives and false negatives (FN):

\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

The F1 Score is defined as the harmonic mean of precision and recall:

\[
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]

Accuracy is defined as the proportion of correct predictions (TP and TN) out of all the predictions:

\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\]

Balanced Accuracy is defined as the average of Sensitivity (Recall) and Specificity:

\[
\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}
\]

where:
\[
\text{Sensitivity/Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}, \quad
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
\]

These metrics are then saved in a csv file for each test image as well as the overall average metrics on the entire test set.

A test set of 50 images was carefully selected to include edge cases and difficult cases and then consistently labelled in the labelling software Label Studio. The labels are drawn without rotation to ensure the maximum overlap, as the labels are exported to \texttt{txt} format and include only the pixel coordinates, width and  height of the bounding boxes annotated.

The overall results on test set are presented below in Table~\ref{tab:metrics1}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Metric}               & \textbf{Parked} & \textbf{Road} \\ \hline
    Average IoU                  & 0.66         & 0.66              \\ \hline
    Average Balanced Accuracy    & 0.58         & 0.58              \\ \hline
    Average Precision            & 0.70         & 0.90              \\ \hline
    Average Recall               & 0.75         & 0.67              \\ \hline
    Average F1 Score             & 0.59         & 0.54              \\ \hline
    Average Accuracy             & 0.69         & 0.64              \\ \hline
    Average Specificity          & 0.74         & 0.79              \\ \hline
    \end{tabular}
    \caption{Performance Metrics for Road Mask Classification}
    \label{tab:metrics1}
\end{table}

Analyse metrics. Cite similar results in object detection

The current road mask has a few outstanding problems, that are discussed further below.
Firstly, sometimes not all parts of the street names are completely removed due to the watermark at the bottom of the image or in the case of the street name being on the very edge of the image. However this seems to rarely be the cause of misclassifications.
For streets where the street name takes up all the whole width of the street, the area where the street name is written is completely erased (turned to black), which can cause cars on the road to be misclassified as parked.
A key issue, previously discussed is that the Mapbox Streets images do not correctly reflect the width of the road for roads with multiple lanes, which leads to cars on the road being misclassified as parked for the furthest out lanes.
This issue was resolved for motorways as those roads are depicted in orange or yellow, therefore it was possible to distort the road on the mask to cover more surface. However doing this for the regular roads, would cause far more misclassifications, then fix the current ones.
Another problem that is inherently due to the Mapbox Streets images is the Dublin Tunnel being marked on the images making it appear above ground even though it is underground, which leads to misclassiifcations in that specific area.

Some images from the test set can be seen below, in Table~\ref{tab:test_images1}.
The predicted parked cars are drawn in red while the true labels are drawn in light pink. The predicted on the road cars are drawn in blue while the true labels are drawn in cyan.

\begin{table}[htbp]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.4\textwidth]{images/image1_road_mask_classification_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image2_road_mask_classification_test_set.png} \\
        \includegraphics[width=0.4\textwidth]{images/image3_road_mask_classification_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image4_road_mask_classification_test_set.png} \\
        \multicolumn{2}{c}{\includegraphics[width=0.4\textwidth]{images/image5_road_mask_classification_test_set.png}} \\
    \end{tabular}
    \caption{Images from the Road Mask Classification Test Set}
    \label{tab:test_images1}
\end{table}

\newpage{}

\subsubsection{Evaluation of the empty parking detection}
The empty parking detection logic is evaluated in the \texttt{evaluate\_empty\_parking\_detection.py} Python script.
The classification is evaluated on the following metrics: Average IoU, Average Precision, Average Recall, Average F1 Score, Average Orientation Accuracy, Average Spot Detection Ratio (SDR), Average Spot Detection Error (SDE), Average False Positive Rate (FPR), and Average False Negative Rate (FNR).

The additional metrics are defined as follows.

The Average Orientation Accuracy is defined as the ratio of the number of predictions that correctly identify the orientation (either horizontal or vertical) of the spots on the total number of true spots:

\[
\text{Average Orientation Accuracy} = \frac{\text{Correct Orientation Predictions}}{\text{Total True Labels}}
                                    = \frac{N_{\text{correct\_orientation}}}{N_{\text{true\_labels}}}
\]

where \( N_{\text{correct\_orientation}} \) represents the number of predictions with correct orientation, and \( N_{\text{true\_labels}} \) represents the total number of true labels.

The Spot Detection Ratio is defined as the ratio of the number of predictions and the total number of true labels:

\[
\text{SDR} = \frac{N_{\text{predictions}}}{N_{\text{total\_true\_labels}}}
\]

where \( N_{\text{predictions}} \) is the total number of detected spots, and \( N_{\text{total\_true\_labels}} \) is the total number of true labels.

The Spot Detection Error is defined as the absolute difference between the number of predictions and the total number of true labels:

\[
\text{SDE} = |N_{\text{predictions}} - N_{\text{total\_true\_labels}}|
\]

The False Positive Rate is defined as the proportion of incorrectly predicted positives (FP) out of all the actual negatives:

\[
\text{Average FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\]

The False Negative Rate is defined as the proportion of incorrectly predicted negatives (FN) out of all the actual positives:

\[
\text{Average FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}}
\]

These metrics are then saved in a csv file for each test image as well as the overall average metrics on the entire test set.

Likewise a test set of 50 images was carefully selected and consistently labelled in Label Studio, without rotation to ensure the maximum overlap. The pixel coordinates, width and height of the annotated bounding boxes are exported. The orientation of each bounding box, is calculated automatically in the evaluation script when loading in the true labels.
Achieving a high IoU was a significant challenge, specifically in this section of the model, as the positioning of the spots can be difficult to label accurately. Indeed, it was difficult to identify if there was sufficient space available to fit a car.
To remedy this issue, the IoU threshold was lowered to 0.35, as there was still significant overlap visible when analysing the test images.
The problems linked to the overlap, is equally due to the differences in orientation as the true labels and the predictions are calculated differently.
For the true labels, the orientation is based on whether the width or length of the spot is larger (the spots with larger width are labelled as horizontal and the spots with larger length are labelled as vertical), while for the model's predictions the orientation is based on the angle measurement.

The overall results on test set are presented below in Table~\ref{tab:metrics2}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Metric}                            & \textbf{Value} \\ \hline
    Average IoU                                & 0.59           \\ \hline
    Average Precision                          & 0.77           \\ \hline
    Average Recall                             & 0.70           \\ \hline
    Average F1 Score                           & 0.69           \\ \hline
    Average Orientation Accuracy               & 0.64           \\ \hline
    Average Spot Detection Ratio (SDR)         & 1.05           \\ \hline
    Average Spot Detection Error (SDE)         & 1.44           \\ \hline
    Average False Positive Rate (FPR)          & 0.23           \\ \hline
    Average False Negative Rate (FNR)          & 0.22           \\ \hline
    \end{tabular}
    \caption{Performance Metrics for Empty Parking Detection}
    \label{tab:metrics2}
\end{table}

Analyse metrics. Cite similar results in object detection.

The main reason empty parking spots are not detected is due to the fact that cars are classed as on the road by the road mask, therefore given that the empty parking detection logic only fills in gaps between parked cars it cannot find the empty spots.
Furthermore, some spots are too small to be found by the model given the threshold set for the average width and average height.
Other minor issues that sometimes lead to empty parking spots not being detected are due to angle misalignment, when one car is oriented strangely??? leading to the angle between the 2 cars being larger that the threshold or when 2 cars are too far apart to fill in the gap.

A few images from the test set are displayed below, in Table~\ref{tab:test_images2}.
The predicted empty parking spots are drawn in green while the true labels are in light pink.

\begin{table}[htbp]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.4\textwidth]{images/image1_empty_parking_detection_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image2_empty_parking_detection_test_set.png} \\
        \includegraphics[width=0.4\textwidth]{images/image3_empty_parking_detection_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image4_empty_parking_detection_test_set.png} \\
        \includegraphics[width=0.4\textwidth]{images/image5_empty_parking_detection_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image6_empty_parking_detection_test_set.png} \\
    \end{tabular}
    \caption{Images from the Empty Parking Detection Test Set}
    \label{tab:test_images2}
\end{table}

\newpage{}

\subsubsection{Evaluation of the classification of the spots detected into private, public and parking lot}
The classification of the spots detected into private, public and parking lot is evaluated in the \texttt{evaluate\_classification\_of\_spots.py} Python script.
The classification is evaluated on the following metrics as defined previously: Average IoU, Balanced Accuracy and Precision, Recall, F1 Score, Accuracy, Specificity per class. These metrics are saved in a csv file for each test image as well as the overall average metrics on the entire test set.

Similarly, a test set of 50 images was carefully selected and consistently labelled in Label Studio, without rotation to ensure the maximum overlap.

The overall results on test set are presented below in Table~\ref{tab:metrics3}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Metric}               & \textbf{Public} & \textbf{Private} & \textbf{Parking Lot} \\ \hline
    Average IoU                  & 0.60             & 0.60             & 0.60              \\ \hline
    Average Balanced Accuracy    & 0.57             & 0.57             & 0.57              \\ \hline
    Average Precision            & 0.73             & 0.65             & 0.59              \\ \hline
    Average Recall               & 0.72             & 0.75             & 0.74              \\ \hline
    Average F1 Score             & 0.68             & 0.62             & 0.61              \\ \hline
    Average Accuracy             & 0.67             & 0.54             & 0.63              \\ \hline
    Average Specificity          & 0.59             & 0.54             & 0.41              \\ \hline
    \end{tabular}
    \caption{Performance Metrics for Parking Spot Classification}
    \label{tab:metrics3}
\end{table}

Analyse metrics. Cite similar results in object detection.

The main outstanding problem for the classification is residential areas which are very rarely misclassified as parking lot with the current thresholds set.
In cases where public spots are quite far from the road, they are classified as private, though that is not a major issue.
Another thing to note is that smaller parking lots with less than 18 spots are not detected though that is intentionally set (by design) in order to focus on larger parking lots and avoid misclassification.

A few images from the test set are shown below, in Table~\ref{tab:test_images3}.
The predicted public spots are drawn in green while the true labels are in dark green. The predicted private spots are highlighted in red while the true labels are in light pink. The predicted parking lot spots are shown in blue while the true labels are in cyan.


\begin{table}[htbp]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.4\textwidth]{images/image1_classification_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image2_classification_test_set.png} \\
        \includegraphics[width=0.4\textwidth]{images/image3_classification_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image4_classification_test_set.png} \\
        \includegraphics[width=0.4\textwidth]{images/image5_classification_test_set.png} & \includegraphics[width=0.4\textwidth]{images/image6_classification_test_set.png} \\
    \end{tabular}
    \caption{Images from the Parking Spot Classification Test Set}
    \label{tab:test_images3}
\end{table}

\newpage{}