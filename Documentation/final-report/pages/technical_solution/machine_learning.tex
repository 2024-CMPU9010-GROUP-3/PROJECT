\subsubsection{Data collection process}
Satellite images collected using Mapbox (Mapbox Streets and Mapbox Satelite)


\subsubsection{Yolo model used for car detection}


\subsubsection{Parking spot detection}
To identify the parking spots from the cars detected by the Yolo model, the cars are classified into on the road or parked, based on a road mask.

\subsubsection{Road mask generation}
Different iterations of the road mask were used, the main iterations and their differences are listed below.
The original mask was a very simple binary mask which identified the road pixels by their color, as the majority of roads are depicted in white.
Then through testing and analysing the Mapbox street view, we discovered certain motorways or national roads were shown in yellow or orange, so 2 additional color masks were concatenated to the original binary wise through bitwise operations.
Given that the Mapbox street view contains additional annotations like street names and white dotted lines which denote (foot path, planter boxes...), the roads aren't that nicely represented by the mask.
So morphological operations were used to remove the street names (and leave the road just in white) as well as the other annotations.
Different kernels sizes and parameters (different contour thresholds) were tested to achieve the removal of the street names.
A different approach using Canny edge was tested, however due to the nature of the Mapbox street view, Canny edge wasn't able to find the correct edges and was picking up unimportant information, performing overall much worse than the current mask at the time.
Then a final addition to the mask was made to avoid certain misclassification in motorways as the road's width was not correctly reflected on the mapbox street view. The road was enlarged using dilation technique, testing with different kernel sizes, different number of iterations and kernels of different sizes used consecutively.

Add a lot more details, all the different parameters and explain how i did testing.

\subsubsection{Classifiction into on the road vs parked}
The cars found by the yolo model are the sorted in on the road or off the road based on the road mask.
The bounding box of the car is computed(we get center, width, height) and if there is overlap between the road pixels by a threshold of 0.5, we discard the model's prediction and only keep the parked cars.
Different thresholds were tested, but overall it was better to have a harsher threshold as these detections are used later on for the empty parking detection.

Expliquer all the coordinates system and the calculations of the center images
Also difference entre parking detection and local parking detection files
Expliquer toutes les fonctions, all the different iterations, what I changed and then added later.
Sauter une ligne ajoute un nouveau paragraphe

\subsubsection{Empty parking spot detection}



\subsubsection{Classification of all spots}

