\subsubsection{Data collection process}
The Mapbox Static Images API was used to retrieve all the satellite images used in this project.
Initially, a training set of 250 satellite images was collected to train the Yolo model.

Our project has two main Python scripts, parking_detection.py and parking_detection_local.py to localize parking spots.
In parking_detection.py, the satellite images (Mapbox Satellite) and corresponding road mask images (Mapbox Streets) are retrieved for a specific area contained within a bounding box, defined by its top-left and bottom-right coordinates, directly from the Mapbox API.
From the coordinates of the specific bounding box, the center coordinates of all the images necessary to make up that area are calculated, and the images are then retrieved.
While in parking_detection_local.py, the original parking detection script is adapted to run on a database of locally stored Mapbox Satellite images and the coresponding Mapbox Streets images, covering the entirety of Dublin city.

\subsubsection{Yolo model used for car detection}


\subsubsection{Parking spot detection}
Subsequently, to identify the parking spots in each satellite image, the cars detected by the Yolo model are classified into on the road or parked, based on a road mask.

\subsubsection{Road mask generation}
Many different iterations of the road mask were used, the main iterations and their differences are explained and showed below.
The original mask was a very simple binary mask which identified the road pixels by their color, as the majority of roads were depicted in white and darkened the rest of the pixels in the image.
Through testing and thoroughly analysing the Mapbox Streets images, motorways and national roads were discovered to be depicted in yellow or orange. Therefore, two additional color masks (for orange and yellow) were concatenated to the original binary mask through bitwise operations.
To refine the mask, and reduce misclassifications, additional annotations contained on the Mapbox Streets images such as street names and white dotted lines which denote a multitude of things (foot paths, planter boxes, pedestrian crosswalks, football fields delimitations ...) were removed through morphological operations, leaving only a plain white line for each road.
A number of different kernel sizes, different contor thresholds and other parameters were tested to achieve the optimal removal of the street names and additonal annotations.
A different approach using Canny Edge Detection was tested, however due to the nature of the Mapbox Streets images, the edges picked up were not significant and the overall perfermances was much worse than the current mask at the time.
A final improvement to the mask was made to avoid certain misclassification due to the Mapbox Streets road width not correctly reflecting all the lanes of the road and therfore classifying on the road cars as parked. This issue was most prominent for motorways and national roads and was resolved by enlarging those roads using a dilation technique. Multiple different kernel sizes, different number of iterations and kernels of different sizes applied consecutively were tested to find the optimal solution.


\subsubsection{Classifiction of cars into on the road or parked}
The cars detected by the Yolo model are then sorted into on the road or parked based on the road mask previously generated, which is explained in more detail below.
The model's predictions which are lower than the confidence threshold of 0.4 are discarded as they are more likely to be misclassifications (such as chimneys which were often misclassified).
A bounding box for each of the model's predictions is computed from the center pixel coordinates, the width and the height returned from the model.
The overlap between the bounding box and the road pixels is calculated, and if the overlap is higher than the threshold of 0.5, the prediction is discarded, keeping only the parked cars.
Different thresholds were tested as well, however overall a harsher threshold works better given that these detections are used later on for the empty parking detection.
For each of the model's predictions for parked cars, the center pixels coordinates are mapped to geographic coordinates (longitude and latitude), and the width, height, roation and orientation based on the angle are saved.
For testing and debugging purposes, all the bounding boxes of the cars found by the model are drawn onto the satellite image, on the road cars are drawn in blue while parked cars are drawn in red as seen in Figure ...

\subsubsection{Empty parking spot detection}
Average parking spot width and length in pixels are calculated and average parking spot width and length in meters is set to 3.05 as found through testing to be the optimal value.
Empty spots in a group of parked cars are detected in each image, based on the parked cars detected.
Originally 4 cases were considered, horizontal in a row, horizontal stacked, vertical in a column and vertical side by side. However horizontal stacked and vertical side by side were removed as they lead to too many misclassifications (cars in driveways...).
The cars are sorted by long/lat and orientation horizontal/vertical. For each consequative spot the distance in meters is calculated, if there is enough space, higher than threshold and the angle deviation is smaller than a certain threshold, the number of cars that fit is claculated and then added to the empty spots detected.
Say for the half spots that are removed on both sides to account for the cars + how the coords are claculated.
Duplicate spots (spots that overlapp too much, based on an optimal threshold, found during testing) and spots coinciding with cars identified by the Yolo model are removed afterwards.
Empty spots that are on the road are also filter out.
Then all the empty spots found are drawn onto the image in green.

\subsubsection{Classification of all spots detected}
The parking spots detected are then classified into public (on the street parking), private (residential parking) and parking lot, based on their proximity to the nearest road.
Parking spots are considered public if the proximity to the nearest road is less than a pixel threshold of 30, otherwise they are considered private. This optimal threshold was found through extensive testing.
Multiple different clustering approaches(HDBSCAN...) were tested to cluster spots together, overall DBSCAN performed the best with it's optimal parameters eps:55 and min_samples:5.
Parking lots are classified as clusters with 18 spots or more.
Clusters are drawn color coded as a circle at the center of the bounding box, and the classification label is written to the right of the bounding box. Private in red, public in green and parking lot in white.

The longitude, latitude and classification of all parking spots detected are then saved in a csv file.
Potential duplicates are dropped, as some cars many be in multiple images as there is overlap between the images(expliquer miex)
The parking zones and costs are added for each file based on a geojson file.

Extensisve testing was done at all stages of the writing of the script (and adding the additonal parts) to find the optimal thresholds/params on test images from Mapbox, in a variety of edge cases like for example residential, urban...



Expliquer clustering technique and params better
Expliquer parking zones and costs mieux en ajoutant la figure du pdf
Expliquer drawing of all bb and classification, code couleur
Expliquer TOUTES les fonctions
Sauter une ligne ajoute un nouveau paragraphe
Explain how i did testing
Add newlines
Refer to each figures, like say as seen in figure x ...
Finish all sections then add all the images