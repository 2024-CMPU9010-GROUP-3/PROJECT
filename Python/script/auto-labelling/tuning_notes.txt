laundry list + notes
30/09
- research more on creating your own image dataset
- brush up on transfer Learning
- in-depth research of other models we are going to use

02/10
- find residential areas and run the script (install pandas & pillow in venv)

08/10
- find way to automatically label using label studio
you did it pookie :) on 09/10 but you did it :) proud of you

10/10
- get the PR approved
- try to further tune the model
- label the rest of the pictures

14/10
- it5 of the model uses hyp.yaml, accuracy went from 0.79 to 0.81
- it6 i will reduce the Learning rate from 0.01 to 0.001 and use it5 weights = acc went frm 0.81 to 0.82
- it7 will put 20 epochs + use weights from it6 = acc went from 0.82 to 0.84
- it8 will keep 20 epochs and changing iou threshold from 0.2 to 0.4 + use weights from it7 = acc went from 0.84 to 0.86
- it9 will keep 20 epochs, changing warm up epochs to 5 instead of 4 + use weights from it8 = same results as it8
- it10 i will reduce epochs back to 10, warmup to 2, add 5.0 degrees img rotation and use it8 weights = same results as it9
- it11 i will keep epochs at 10, increase degrees to 7.0, decrease scale to 0.3 (parameters focused on image augmentation) & cls to 1.5 = same but p curve worse than previous iteration. img augmentation did Nothing.
- it12 i will keep 10 epochs, put the img aug param back, keep cls at 1.5 and increase box loss from 0.05 to 0.1; will use it8 weights = acc decreased to 0.85, p curve as bad as it11, i think its because of the class loss
- it13 i will put cls back to 1.0, but box loss back to 0.05 and reduce mosaic augmentation from 1.0 to 0.5 = same results, trying now with mosaic 0 and i will overwrite it13 = acc same but pr mAP shows slight decrease, no overwrite

wtf do i do

15/10
- i think i will try to use the yolov8 labels again because the first iteration yielded very good acc (0.86) in the first run without hyp +
- moved to using the yolov8 model, wrote the script and ran the model for 20 epochs-16 batch on the "n" model = acc at 0.59
- switch to the "s" model = acc at 0.61, will use this moving forward

16/10
- keeping "s" model, adding evolve=True and switching to batch=8 for 10 epochs (also realized i was using an empty model for the previous ones, i'm using pre-trained now) = 0.71 acc
- using .tune with 10 iterations-20 epochs-adamw optimizer-batch 16 and no plotting or validation until last it for faster tuning = the fitness/tuning plot showed no improvement overtime, so Saul is running overnight for 100 itérations + val=True for hopefully better results.

17/10
- looking at the fitness plot, the tuning did help find an ideal set of parameters, waiting for saul to run a new training now with these weights
- training with the new weights, 30 epochs-patience 10-seed 1-batch 16-cache "disk" = 0.88 acc which is promising
- training with same weights for 50 epochs-patience 20 etc... = 0.86 acc, which decreased. tbh before i hadnt put seed param so reproducibility was not there, now it is. i'm not sure what to change this time around to final cross into 0.90 territory
- switched to the m model with the fine tuned weights from s model, no improvement, therefore Saul with run tuning again tonight on the m model and we'll retrain tomorrow with hopefully more appropriate weights
- decided to do one last training of the m model with val=true and optimizer=adamw to see if anything improves = 0.83, yeah we will go with fine-tuning.
- in the first fine-tune, i think i set way too many itérations relative to the size of my dataset and batch size (data_size / batch size = n itérations) so i'm reducing it accordingly and hopefully we get better results tonight.

# notes on yolo v8 model
"n" - nano, smallest & lightest model out there optimized for speed and lower resources
"s" - small, larger than n, slightly less accurate than bigger models
"m" - medium, increased resource usage compared to s and n
"l" - large, designed for higher accuracy an more resource usage
"x" - extra large, more powerful, best for projects requiring high accuracy but very resource-intensive

YOLOv8 Model Variants and Specifications:
Model	Parameters	Layers	Size (Memory)	GFLOPs	Speed (ms)	Usage Recommendation
YOLOv8n (Nano)	~3.2M	128	~7 MB	~8.7	Fastest	Best for mobile/edge devices where speed is critical but accuracy can be sacrificed.
YOLOv8s (Small)	~11.2M	192	~20 MB	~26.2	Fast	Good for general applications, a balance of speed and accuracy. Suitable for real-time applications.
YOLOv8m (Medium)	~25.9M	256	~60 MB	~78.9	Medium	Suitable for higher accuracy tasks, without needing extreme resource efficiency.
YOLOv8l (Large)	~43.7M	320	~99 MB	~155.6	Slower	Best for tasks where higher accuracy is required and more computational power is available.
YOLOv8x (Extra Large)	~68.2M	384	~168 MB	~257.8	Slowest	Highest accuracy model, typically used for server-side applications or research.

